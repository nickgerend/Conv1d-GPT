{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d-GPT: Chatty Philosopher 1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1d-GPT: Nick Gerend, Jan 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train import Conv1dGPT, get_batch, estimate_loss, configure_optimizer\n",
    "import os\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}']\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "with open('philosophy.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple nanoGPT (Karpathy) style setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to token integers\n",
    "str_out_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_out_str = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [str_out_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_out_str[i] for i in l])\n",
    "\n",
    "# train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# 90% split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'batch_size': 64 # independent sequences to parallel process\n",
    "    ,'block_size': 256 # (max) context length for predictions\n",
    "    ,'max_iters': 3000 # training loop updates\n",
    "    ,'eval_interval': 50 # loss report and checkpoint interval\n",
    "    ,'learning_rate': 3e-3 # initial leanring rate\n",
    "    ,'device': 'cuda' if torch.cuda.is_available() else 'cpu' # 1 RTX 4080 GPU here\n",
    "    ,'eval_iters': 200 # samples to evaluate\n",
    "    ,'n_embd': 128 # embedding dimensions\n",
    "    ,'n_head': 6 # attention heads\n",
    "    ,'n_layer': 7 # Conv1d-GPT block layers\n",
    "    ,'dropout': 0.1 # dropout rate for select layers\n",
    "    ,'kernel_size': 3 # Conv1d kernel size (dilate factor is 2**blocks so check against n_layer)\n",
    "    ,'conv': True # enable Conv1d, set to False to test performance of boiler-plate GPT\n",
    "    ,'weight_decay': 1e-1 # learning rate decay\n",
    "    ,'betas': (0.9, 0.95) # learning rate betas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d-GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.778905 M parameters\n",
      "Loss at step 0: 4.524252891540527\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "model = Conv1dGPT(vocab_size, hyp['n_embd'], hyp['block_size'], hyp['n_head'], hyp['dropout'],\n",
    "    hyp['conv'], hyp['kernel_size'], hyp['n_layer'], hyp['device']).to(hyp['device'])\n",
    "\n",
    "# parameter count\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# initial loss\n",
    "xb, yb = get_batch(train_data, val_data, 'train', hyp['block_size'], hyp['batch_size'], hyp['device'])\n",
    "logits, loss = model(xb, yb)\n",
    "print(\"Loss at step 0:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nanoGPT (Karpathy) style optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 157, with 1,768,704 parameters\n",
      "num non-decayed parameter tensors: 59, with 10,201 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "def configure_optimizer(model, weight_decay, learning_rate, betas, device_type):\n",
    "    # start with all of the candidate parameters\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    # filter out those that do not require grad\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "    # create optim groups, any parameters that is 2D will be weight decayed, otherwise no\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {'params': decay_params, 'weight_decay': weight_decay},\n",
    "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "    # create AdamW optimizer and use the fused version if it is available\n",
    "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device_type == 'cuda'\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "    print(f\"using fused AdamW: {use_fused}\")\n",
    "    return optimizer\n",
    "\n",
    "optimizer = configure_optimizer(model, hyp['weight_decay'], hyp['learning_rate'], hyp['betas'], hyp['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5251, val loss 4.5252\n",
      "step 50: train loss 2.3308, val loss 2.3725\n",
      "step 100: train loss 2.0006, val loss 2.0542\n",
      "step 150: train loss 1.7698, val loss 1.8474\n",
      "step 200: train loss 1.6395, val loss 1.7166\n",
      "step 250: train loss 1.5410, val loss 1.6125\n",
      "step 300: train loss 1.4893, val loss 1.5587\n",
      "step 350: train loss 1.4476, val loss 1.5223\n",
      "step 400: train loss 1.4100, val loss 1.4808\n",
      "step 450: train loss 1.3815, val loss 1.4632\n",
      "step 500: train loss 1.3653, val loss 1.4438\n",
      "step 550: train loss 1.3484, val loss 1.4242\n",
      "step 600: train loss 1.3325, val loss 1.4143\n",
      "step 650: train loss 1.3191, val loss 1.3999\n",
      "step 700: train loss 1.3047, val loss 1.3904\n",
      "step 750: train loss 1.2965, val loss 1.3779\n",
      "step 800: train loss 1.2861, val loss 1.3683\n",
      "step 850: train loss 1.2805, val loss 1.3700\n",
      "step 900: train loss 1.2711, val loss 1.3554\n",
      "step 950: train loss 1.2634, val loss 1.3540\n",
      "step 1000: train loss 1.2565, val loss 1.3447\n",
      "step 1050: train loss 1.2523, val loss 1.3418\n",
      "step 1100: train loss 1.2491, val loss 1.3333\n",
      "step 1150: train loss 1.2425, val loss 1.3344\n",
      "step 1200: train loss 1.2378, val loss 1.3286\n",
      "step 1250: train loss 1.2320, val loss 1.3232\n",
      "step 1300: train loss 1.2305, val loss 1.3211\n",
      "step 1350: train loss 1.2270, val loss 1.3124\n",
      "step 1400: train loss 1.2218, val loss 1.3102\n",
      "step 1450: train loss 1.2183, val loss 1.3069\n",
      "step 1500: train loss 1.2141, val loss 1.3049\n",
      "step 1550: train loss 1.2085, val loss 1.3011\n",
      "step 1600: train loss 1.2062, val loss 1.3069\n",
      "step 1650: train loss 1.2014, val loss 1.2937\n",
      "step 1700: train loss 1.2004, val loss 1.2976\n",
      "step 1750: train loss 1.1973, val loss 1.2946\n",
      "step 1800: train loss 1.2007, val loss 1.2975\n",
      "step 1850: train loss 1.1949, val loss 1.2886\n",
      "step 1900: train loss 1.1887, val loss 1.2824\n",
      "step 1950: train loss 1.1886, val loss 1.2805\n",
      "step 2000: train loss 1.1860, val loss 1.2827\n",
      "step 2050: train loss 1.1824, val loss 1.2757\n",
      "step 2100: train loss 1.1813, val loss 1.2797\n",
      "step 2150: train loss 1.1811, val loss 1.2763\n",
      "step 2200: train loss 1.1797, val loss 1.2746\n",
      "step 2250: train loss 1.1752, val loss 1.2721\n",
      "step 2300: train loss 1.1716, val loss 1.2688\n",
      "step 2350: train loss 1.1710, val loss 1.2765\n",
      "step 2400: train loss 1.1679, val loss 1.2648\n",
      "step 2450: train loss 1.1682, val loss 1.2657\n",
      "step 2500: train loss 1.1641, val loss 1.2616\n",
      "step 2550: train loss 1.1650, val loss 1.2629\n",
      "step 2600: train loss 1.1632, val loss 1.2598\n",
      "step 2650: train loss 1.1594, val loss 1.2623\n",
      "step 2700: train loss 1.1596, val loss 1.2623\n",
      "step 2750: train loss 1.1555, val loss 1.2574\n",
      "step 2800: train loss 1.1533, val loss 1.2507\n",
      "step 2850: train loss 1.1545, val loss 1.2526\n",
      "step 2900: train loss 1.1553, val loss 1.2561\n",
      "step 2950: train loss 1.1507, val loss 1.2518\n",
      "step 2999: train loss 1.1517, val loss 1.2495\n"
     ]
    }
   ],
   "source": [
    "# training loop, ~178 minutes\n",
    "for iter in range(hyp['max_iters']):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % hyp['eval_interval'] == 0 or iter == hyp['max_iters'] - 1:\n",
    "        losses = estimate_loss(model, train_data, val_data, hyp['eval_iters'], hyp['block_size'], hyp['batch_size'], hyp['device'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': hyp,\n",
    "            'iter_num': iter,\n",
    "            'val_loss': losses['val'],\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join('./conv1d_gpt_checkpoints', 'ckpt_i' + str(iter) + f\"_v{losses['val']:.4f}\".replace('.','_') + '.pt'))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(train_data, val_data, 'train', hyp['block_size'], hyp['batch_size'], hyp['device'])\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatty Philosopher (And so,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And so, the Cerecular life wide degrees will ask\n",
      "the rat!-Mase chalicies have night, is other\n",
      "times, and Still? For it definite thoughtanging\n",
      "perashed by another, now pursues it to clum times\n",
      "bescures, precius, and usufirs to observe enlark\n",
      "contributions where to advance in pronounced\n",
      "oneself, the scientification as an inseparable he\n",
      "from the necessitys of a sufficien, and a\n",
      "certainment him, for us impublic scobling world of\n",
      "acdocrance with incidence need:\" And the seat are\n",
      "made predominan his partedne...\n"
     ]
    }
   ],
   "source": [
    "# Conv1-GPT: 3K iters, train loss 1.1517, val loss 1.2495\n",
    "cp = decode(model.generate_and_so(max_new_tokens=500)[0].tolist())\n",
    "width = 50\n",
    "print('\\n'.join(textwrap.wrap(cp + '...', width=width)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3_11_8_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
