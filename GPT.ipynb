{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT: Chatty Philosopher 1.8M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train import Conv1dGPT, get_batch, estimate_loss, configure_optimizer\n",
    "import os\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}']\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "with open('philosophy.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple nanoGPT (Karpathy) style setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to token integers\n",
    "str_out_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_out_str = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [str_out_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_out_str[i] for i in l])\n",
    "\n",
    "# train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# 90% split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'batch_size': 64 # independent sequences to parallel process\n",
    "    ,'block_size': 256 # (max) context length for predictions\n",
    "    ,'max_iters': 3000 # training loop updates\n",
    "    ,'eval_interval': 50 # loss report and checkpoint interval\n",
    "    ,'learning_rate': 3e-3 # initial leanring rate\n",
    "    ,'device': 'cuda' if torch.cuda.is_available() else 'cpu' # 1 RTX 4080 GPU here\n",
    "    ,'eval_iters': 200 # samples to evaluate\n",
    "    ,'n_embd': 128 # embedding dimensions\n",
    "    ,'n_head': 6 # attention heads\n",
    "    ,'n_layer': 9 # GPT block layers (increase from 7 to get up to about the same parameter count)\n",
    "    ,'dropout': 0.1 # dropout rate for select layers\n",
    "    ,'kernel_size': 3 # Conv1d kernel size (dilate factor is 2**blocks so check against n_layer)\n",
    "    ,'conv': False # enable Conv1d, set to False to test performance of boiler-plate GPT\n",
    "    ,'weight_decay': 1e-1 # learning rate decay\n",
    "    ,'betas': (0.9, 0.95) # learning rate betas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Model (A/B test to Conv1d Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.827673 M parameters\n",
      "Loss at step 0: 4.515688896179199\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "model = Conv1dGPT(vocab_size, hyp['n_embd'], hyp['block_size'], hyp['n_head'], hyp['dropout'],\n",
    "    hyp['conv'], hyp['kernel_size'], hyp['n_layer'], hyp['device']).to(hyp['device'])\n",
    "\n",
    "# parameter count\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# initial loss\n",
    "xb, yb = get_batch(train_data, val_data, 'train', hyp['block_size'], hyp['batch_size'], hyp['device'])\n",
    "logits, loss = model(xb, yb)\n",
    "print(\"Loss at step 0:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nanoGPT (Karpathy) style optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 192, with 1,815,808 parameters\n",
      "num non-decayed parameter tensors: 66, with 11,865 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "optimizer = configure_optimizer(model, hyp['weight_decay'], \n",
    "    hyp['learning_rate'], hyp['betas'], hyp['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5177, val loss 4.5176\n",
      "step 50: train loss 2.8095, val loss 2.8280\n",
      "step 100: train loss 2.5179, val loss 2.5457\n",
      "step 150: train loss 2.4401, val loss 2.4713\n",
      "step 200: train loss 2.4072, val loss 2.4400\n",
      "step 250: train loss 2.3899, val loss 2.4239\n",
      "step 300: train loss 2.3531, val loss 2.3888\n",
      "step 350: train loss 2.2428, val loss 2.2830\n",
      "step 400: train loss 2.1300, val loss 2.1790\n",
      "step 450: train loss 2.0325, val loss 2.0813\n",
      "step 500: train loss 1.9345, val loss 1.9953\n",
      "step 550: train loss 1.8477, val loss 1.9147\n",
      "step 600: train loss 1.7695, val loss 1.8478\n",
      "step 650: train loss 1.6994, val loss 1.7831\n",
      "step 700: train loss 1.6548, val loss 1.7301\n",
      "step 750: train loss 1.6049, val loss 1.6789\n",
      "step 800: train loss 1.5571, val loss 1.6367\n",
      "step 850: train loss 1.5299, val loss 1.6088\n",
      "step 900: train loss 1.5041, val loss 1.5750\n",
      "step 950: train loss 1.4758, val loss 1.5455\n",
      "step 1000: train loss 1.4589, val loss 1.5347\n",
      "step 1050: train loss 1.4404, val loss 1.5139\n",
      "step 1100: train loss 1.4173, val loss 1.4972\n",
      "step 1150: train loss 1.4040, val loss 1.4867\n",
      "step 1200: train loss 1.3896, val loss 1.4616\n",
      "step 1250: train loss 1.3748, val loss 1.4500\n",
      "step 1300: train loss 1.3680, val loss 1.4489\n",
      "step 1350: train loss 1.3624, val loss 1.4326\n",
      "step 1400: train loss 1.3528, val loss 1.4315\n",
      "step 1450: train loss 1.3437, val loss 1.4274\n",
      "step 1500: train loss 1.3311, val loss 1.4071\n",
      "step 1550: train loss 1.3260, val loss 1.4044\n",
      "step 1600: train loss 1.3253, val loss 1.4069\n",
      "step 1650: train loss 1.3127, val loss 1.3920\n",
      "step 1700: train loss 1.3068, val loss 1.3813\n",
      "step 1750: train loss 1.3012, val loss 1.3816\n",
      "step 1800: train loss 1.2973, val loss 1.3718\n",
      "step 1850: train loss 1.2938, val loss 1.3770\n",
      "step 1900: train loss 1.2838, val loss 1.3606\n",
      "step 1950: train loss 1.2841, val loss 1.3647\n",
      "step 2000: train loss 1.2770, val loss 1.3587\n",
      "step 2050: train loss 1.2715, val loss 1.3552\n",
      "step 2100: train loss 1.2707, val loss 1.3520\n",
      "step 2150: train loss 1.2637, val loss 1.3426\n",
      "step 2200: train loss 1.2639, val loss 1.3448\n",
      "step 2250: train loss 1.2644, val loss 1.3433\n",
      "step 2300: train loss 1.2598, val loss 1.3387\n",
      "step 2350: train loss 1.2592, val loss 1.3392\n",
      "step 2400: train loss 1.2487, val loss 1.3326\n",
      "step 2450: train loss 1.2461, val loss 1.3283\n",
      "step 2500: train loss 1.2485, val loss 1.3336\n",
      "step 2550: train loss 1.2436, val loss 1.3258\n",
      "step 2600: train loss 1.2386, val loss 1.3246\n",
      "step 2650: train loss 1.2382, val loss 1.3262\n",
      "step 2700: train loss 1.2409, val loss 1.3260\n",
      "step 2750: train loss 1.2331, val loss 1.3184\n",
      "step 2800: train loss 1.2312, val loss 1.3204\n",
      "step 2850: train loss 1.2305, val loss 1.3169\n",
      "step 2900: train loss 1.2259, val loss 1.3130\n",
      "step 2950: train loss 1.2273, val loss 1.3118\n",
      "step 2999: train loss 1.2186, val loss 1.3101\n"
     ]
    }
   ],
   "source": [
    "# training loop, ~218 minutes\n",
    "for iter in range(hyp['max_iters']):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % hyp['eval_interval'] == 0 or iter == hyp['max_iters'] - 1:\n",
    "        losses = estimate_loss(model, train_data, val_data, hyp['eval_iters'], hyp['block_size'], hyp['batch_size'], hyp['device'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': hyp,\n",
    "            'iter_num': iter,\n",
    "            'val_loss': losses['val'],\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join('./gpt_checkpoints', 'ckpt_i' + str(iter) + f\"_v{losses['val']:.4f}\".replace('.','_') + '.pt'))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(train_data, val_data, 'train', hyp['block_size'], hyp['batch_size'], hyp['device'])\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatty Philosopher (And so,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And so, fron so to help.      Who may independ it\n",
      "found no louds of remains be that only thrat about\n",
      "to datom the will, if the irrrelige town rarguar\n",
      "equal. If any proporiation, be short of child\n",
      "generical existence at miseriably civil to\n",
      "applaray portemportion of expressives are true\n",
      "forbid and Steduch, thereby the pores of concape,\n",
      "and their case understood and him, more into a\n",
      "proficies of sculpture, matheme each and sen\n",
      "degution with it? Own false at they harms with\n",
      "away him priora for a conception...\n"
     ]
    }
   ],
   "source": [
    "# GPT: 3K iters, train loss , val loss \n",
    "cp = decode(model.generate_and_so(max_new_tokens=500)[0].tolist())\n",
    "width = 50\n",
    "print('\\n'.join(textwrap.wrap(cp + '...', width=width)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3_11_8_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
